Why Docker?

Docker/containers are important for a few reasons - 

    - Kubernetes/Container orchestration
    - Running processes in isolated environments
    - Starting projects/auxilary services locally

Containerization -

    Containers are a way to package and distribute software applications in a way that makes them easy to deploy and run consistently across different environments. They allow you to package an application, along with all its dependencies and libraries, into a single unit that can be run on any machine with a container runtime, such as Docker.

Why containers

    - Everyone has different Operating systems
    - Steps to run a project can vary based on OS
    - Extremely harder to keep track of dependencies as project grows

Benefits of using containers

    - Let you describe your configuration in a single file
    - Can run in isolated environments
    - Makes Local setup of Open Source projects a breeze
    - Makes installing auxiliary services/DBs easy

For Reference, the following command starts mongo in all operating systems - 

docker run -d -p 27017:27017 mongo

Docker isn’t the only way to create containers

---

Docker is a YC backed company, started in ~2014
They envisioned a world where containers would become mainstream and people would deploy their applications using them
That is mostly true today
Most projects that you open on Github will/should have docker files in them (a way to create docker containers)
 
Ref - https://www.ycombinator.com/blog/solomon-hykes-docker-dotcloud-interview/


In our machine =>

(docker run mongo) --> docker engine --> docker registry
Terminal
                   <-- done     |     <--
                        Mongo DB container


Terminal / Docker GUI / REST APIs (ways to connect to docker engine) talks to docker engine, which let's us fetch images from the registry (if not already fetched) --> create containers

Images are already written packages that are being pushed to the registry, so that other users can fetch and use.We can run own containers from the images we have created.

As an application/full stack developer, you need to be comfortable with the following terminologies -

    - Docker Engine
    - Docker CLI - Command line interface
    - Docker registry

1. Docker Engine -

Docker Engine is an open-source containerization technology that allows developers to package applications into container.
Containers are standardized executable components combining application source code with the operating system (OS) libraries and dependencies required to run that code in any environment.

2. Docker CLI -

The command line interface lets you talk to the docker engine and lets you start/stop/list containers

docker run -d -p 27017:27017 mongo

Docker cli is not the only way to talk to a docker engine. You can hit the docker REST API to do the same things

3. Docker registry -

The docker registry is how Docker makes money. 

It is similar to github, but it lets you push images rather than sourcecode

Docker’s main registry - https://dockerhub.com/

Mongo image on docker registry - https://hub.docker.com/_/mongo

----

Images vs containers

Docker Image =>

A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files.

A good mental model for an image is Your codebase on github

Docker Container =>

A container is a running instance of an image. It encapsulates the application or service and its dependencies, running in an isolated environment.

A good mental model for a container is when you run node index.js on your machine from some source code you got from github

We can run multiple instances of an image in our machine on different ports.

Port mapping =>

docker run -d -p 27018:27017 mongo

the containers have their own ports, so we need to map our system's port to the container's port in order to access them. i.e => <system_port>:<container_port> - so that any request coming to the machine will get routed to the container's port. Docker by default will run the containers in an isolated environment, so in order to expose some of the functionality via the container ports we need to map them with our machine ports. If applications doesn't need to interact with internet then the containers can just run like that, we don't need mapping for that.


Common docker commands =>

    - docker images
    - docker ps
    - docker run
    - docker build


1. docker images
Shows you all the images that you have on your machine

2. docker ps
Shows you all the containers you are running on your machine

3. docker run
Lets you start a container
-p ⇒ let’s you create a port mapping
-d. ⇒ Let’s you run it in detatched mode

4. docker build
Lets you build an image. We will see this after we understand how to create your own Dockerfile

5. docker push
Lets you push your image to a registry

6. Extra commands

docker kill - kills the container - docker kill <container_id>

remove image => docker rmi <image_name> (--force optional)

docker exec - docker exec -it <container_id / name> <commands> -  to exectue a command inside a container

Examples

List all contents of a container folder =>

docker exec <container_name_or_id> ls /path/to/directory

Running an Interactive Shell=>

docker exec -it <container_name_or_id> /bin/bash

If we don't want to run some command interactively we can ommit the -it flag.

docker start container_name - to start existing container
docker stop container_name - stop existing container

to get out of container command while keeping the container running => ctrl + d



----

Dockerfile =>

What is a Dockerfile

If you want to create an image from your own code, that you can push to dockerhub, you need to create a Dockerfile for your application.

A Dockerfile is a text document that contains all the commands a user could call on the command line to create an image.

How to write a dockerfile =>

A dockerfile has 2 parts
    - Base image

    - Bunch of commands that you run on the base image (to install dependencies like Node.js)


Let’s write our own Dockerfile

Let’s try to containerise this backend app - https://github.com/100xdevs-cohort-2/week-15-live-1

```

---
FROM node:20

WORKDIR /app

COPY . .

RUN npm install
RUN npx prisma generate
RUN npm run build

EXPOSE 3000
--- The above chunk runs when we create an image = bootstrapping the project => setup

CMD ["node", "dist/index.js"] - This runs when we start a image / container

```

Common commands

- WORKDIR: Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY instructions that follow it.

- RUN: Executes any commands in a new layer on top of the current image and commits the results.

- CMD: Provides defaults for executing a container. There can only be one CMD instruction in a Dockerfile.

- EXPOSE: Informs Docker that the container listens on the specified network ports at runtime.

- ENV: Sets the environment variable.

- COPY: Allow files from the Docker host to be added to the Docker image - ignore node_modules and dist

https://github.com/100xdevs-cohort-2/week-15-live-1

Building images =>

Now that you have a dockerfile in your project, try building a docker image from it

docker build -t image_name . => at the end we need t give a .(dot) to let docker know where to build the image from.(in our case the same directory (.) else mention path instead of . (path of Dockerfile))
 
Now if you try to look at your images, you should notice a new image created

docker images
 
Add a .dockerignore so that node_modules don’t get copied over

Running images =>

docker run -p 3000:3000 image_name

Try visiting localhost:3000

Passing in env variables =>

Never commit the .env files or secrets. Neither shall we hardcode the env variables. We should always inject it while starting a container, during execution. Even if we didn't have that in .env, when we inject the env variable prisma client will take it from the CLI.

When we deploy the app in EC2 or something the env variables are put in there so that there won't be any leaks of secrets.

    - docker run -p 3000:3000 -e DATABASE_URL="postgres://avnadmin:password/defaultdb?sslmode=require" image_name

The -e argument let’s you send in environment variables to your node.js app

----

docker containers are lightweight as compared to VMs.

VMs are created on Cloud providers -> They own very large hyper visors(with so much resources) instead of buying large number of machines and from there (hyper-visors) we get a VM (small portion/ shared resources) to run.

Also we should not self host databases, we should use cloud providers for hosting databases. Because our data is persisted there and multiple copies of it are being made so that our data doesn't get lost. Once we kill the container the local database data will just get erased.

-----

if we don't put the .env file and npx prisma generate won't have access to the .env file and DATABASE_URL.

DATABASE_URL="--" node dist/index.js => this is a way to inject environment variables into node js application.

----

Pushing to dockerhub ->

Once you’ve created your image, you can push it to dockerhub to share it with the world.

- Signup to dockerhub
- Create a new repository
- Login to docker cli
    - docker login

you might have to create an access token - https://docs.docker.com/security/for-developers/access-tokens/

- Push to the repository
    - docker push your_username/your_reponame:tagname


Anytime the code changes, we need to rebuild it and then push it to dockerhub.

for local development,we push the .env.example file to github so that a new developer can set it up their local.

for prod development we can inject them so no need.

CMD ["node","",""] => command then we give arguments to it inside the array after the command separated by commas.

----

Layers in Docker =>

In Docker, layers are a fundamental part of the image architecture that allows Docker to be efficient, fast, and portable. A Docker image is essentially built up from a series of layers, each representing a set of differences from the previous layer.

How layers are made - 

- Base Layer: The starting point of an image, typically an operating system (OS) like Ubuntu, Alpine, or any other base image specified in a Dockerfile.

- Instruction Layers: Each command in a Dockerfile creates a new layer in the image. These include instructions like RUN, COPY, which modify the filesystem by installing packages, copying files from the host to the container, or making other changes. Each of these modifications creates a new layer on top of the base layer.

- Reusable & Shareable: Layers are cached and reusable across different images, which makes building and sharing images more efficient. If multiple images are built from the same base image or share common instructions, they can reuse the same layers, reducing storage space and speeding up image downloads and builds.

- Immutable: Once a layer is created, it cannot be changed. If a change is made, Docker creates a new layer that captures the difference. This immutability is key to Docker's reliability and performance, as unchanged layers can be shared across images and containers.


Layers practically -

For a simple Node.js app - https://github.com/100xdevs-cohort-2/week-15-live-2

FROM node:20  --- Layer 1
WORKDIR /usr/src/app   --- Layer 2
COPY . .  --- Layer 3
RUN npm install  --- Layer 4
RUN npm run build  --- Layer 5
RUN npx prisma generate  --- Layer 6


EXPOSE 3000
CMD ["node","dist/index.js"]

Observations - 

- Base image creates the first layer

- Each RUN, COPY , WORKDIR  command creates a new layer

- Layers can get re-used across docker builds (notice CACHED in 1/6)

----

Why layers?

If you change your Dockerfile, layers can get re-used based on where the change was made

If a layer changes, all subsequent layers also change

Case 1 - You change your source code

FROM node:20  --- Layer 1 cached
WORKDIR /usr/src/app  --- Layer 2 cached

COPY . .  --- Layer 3 changed
RUN npm install --- Layer 4 changed
RUN npm run build --- Layer 5 changed
RUN npx prisma generate  --- Layer 6 changed

EXPOSE 3000
CMD ["node","dist/index.js"]

Case 2 - You change the package.json file (added a dependency)

FROM node:20  --- Layer 1 cached
WORKDIR /usr/src/app  --- Layer 2 cached

COPY . .  --- Layer 3 changed
RUN npm install --- Layer 4 changed
RUN npm run build --- Layer 5 changed
RUN npx prisma generate  --- Layer 6 changed

EXPOSE 3000
CMD ["node","dist/index.js"]

Thought experiment =>

How often in a project do you think dependencies change ?
How often does the npm install layer need to change?
Wouldn’t it be nice if we could cache the npm install step considering dependencies don’t change often?

----

Optimising Dockerfile =>

What if we change the Dockerfile a bit - 

FROM node:20

WORKDIR /usr/src/app

------
COPY package* . - since package.json doesn't change often - anything that starts with package copies it to the image
COPY ./prisma . - same for prisma folder - anything inside prisma folder copies it to the image
------
    
RUN npm install - depends on package.json, so only run when there is a change in package.json (any file change that starts with package) else cached.
RUN npx prisma generate - depends on prisma schema, so only run when there is a change in prisma folder else cached.

--------
COPY . . - rest of the source code might change, so we copied the unchanged parts at first in order to maximize our cached layers. - copies over the rest of the source code

RUN npm run build
--------

EXPOSE 3000

CMD ["node", "dist/index.js", ]

--->

We first copy over only the things that npm install and npx prisma generate need

Then we run these scripts

Then we copy over the rest of the source code

Case 1 - You change your source code (but nothing in package.json/prisma)

FROM node:20 -- Layer 1 cached

WORKDIR /usr/src/app  -- Layer 2 cached

------
COPY package* .  -- Layer 3 cached
COPY ./prisma .  -- Layer 4 cached
------
    
RUN npm install  -- Layer 5 cached
RUN npx prisma generate  -- Layer 6 cached

--------
COPY . .  -- Layer 7 uncached
RUN npm run build
--------

EXPOSE 3000

CMD ["node", "dist/index.js", ]

Case 2 - You change the package.json file (added a dependency)

FROM node:20 -- Layer 1 cached

WORKDIR /usr/src/app  -- Layer 2 cached

------
COPY package* .  -- Layer 3 uncached
COPY ./prisma .  -- Layer 4 uncached
------
    
RUN npm install  -- Layer 5 uncached
RUN npx prisma generate  -- Layer 6 uncached

--------
COPY . .  -- Layer 7 uncached
RUN npm run build
--------

EXPOSE 3000

CMD ["node", "dist/index.js", ]

----

If a layer is cached, then everything before that layer is also cached. If a layer is uncached, then everything after that layer become uncached. If we didn't change anything and build again, then every layer will be shown as cached. as nothing is changed so docker will reuse the previously cached image layers.

While working with complex applications, We want high number of layers to be cached and less number of layers to be uncached.since layers can be reused. We want to decrease the build time, by caching the max number of layers cached as possible.

In our day to day work, the package.json / prisma schema remain unchanged unless there is a need for new dependencies. Most of the time we will be working in the src folder and components.

so npm install doesn't need to be run often as it only depends on the package.json.

time of build is not dependent upon the number of layers => so we can optimize the dockerfile by adding layers. there won't be any significant changes in build time.


We shall try to deferr the things that changes the most to be copied towards the end of the dockerfile just before the final build (npm run build). i.e => index.ts, db.ts etc.

expensive unchanged things we shall try to push up the dockerfile to cache.(npm install, npx prisma generate etc)

-----

Networks and volumes =>

Networks and volumes are concepts that become important when you have multiple containers running in which you -

- Need to persist data across docker restarts
- Need to allow containers to talk to each other

```
Backend Container (Express) --- Network --- Database Container (Mongo) --- Volume
```

We didn’t need networks until now because when we started the mongo container, it was being accessed by a Node.js process running directly on the machine. If there are container communication required networks are required there.

```
node index.js ---> Database Container(Mongo) ---> Volume
```

Volumes =>

If you restart a mongo docker container, you will notice that your data goes away. 
This is because docker containers are transitory (they don’t retain data across restarts)

1. Without volumes =>

Start a mongo container locally =>

    - docker run -p 27017:27017 -d mongo

Open it in MongoDB Compass and add some data to it

Kill the container
    - docker kill <container_id>

Restart the container
    - docker run -p 27017:27017 -d mongo

Try to explore the database in Compass and check if the data has persisted (it wouldn’t)

2. With volumes =>

Create a volume
    - docker volume create volume_database

Mount the folder in mongo which actually stores the data to this volume -
    - docker run -v volume_database:/data/db -p 27017:27017 mongo

Open it in MongoDB Compass and add some data to it

Kill the container
    - docker kill <container_id>

Restart the container
    - docker run -v volume_database:/data/db -p 27017:27017 mongo

we need to restart by attaching the same volume.

-v volume_database:/data/db =>
volume_database mounted to /data/db in the container.

for postgres it might differ.for mongodb we need to persist the /data/db folder.

Try to explore the database in Compass and check if the data has persisted (it will!)

```
MongoDB Container [ /data/db ] ====== Volume
```

Folder 1 -
    - Dockerfile
        - FROM node:20 -- layer 1
        - WORKDIR /app -- layer 2
        - ...
        - ...diff lines
        - asd

Folder 2 -
    - Dockerfile
        - FROM node:20 -- layer 1
        - WORKDIR /app -- layer 2
        - ...
        - ...diff lines
        - asd

here layer 1 + layer 2 = cached layers can be reused in different folders while building images. Each layer is being added on top of the cached layer and that is also being cached so that it can be reused in future image builds if necessary.

Even if we have different Dockerfiles in different folders, the layers created while building a image in one folder can be reused in the other folder as well. Docker keeps the track of layers created and whenever they can be reused, it applies to the image. All Dockerfiles can use the cached image from cached layers stored.

Whatever is causing the layers to be uncached, we should move it down the Dockerfile.

If anything in the source code changes, we need to build the image again unless the Dockerfile is written to be hot reloaded.

----

Network =>
 
In Docker, a network is a powerful feature that allows containers to communicate with each other and with the outside world.

Docker containers can’t talk to each other by default.

localhost on a docker container means it's own network and not the network of the host machine

How to make containers talk to each other?

    - Attach them to the same network

Clone the repo - https://github.com/100xdevs-cohort-2/week-15-live-2.2

Build the image
    - docker build -t image_tag .

Create a network
    - docker network create my_custom_network

Start the backend process with the network attached to it
    - docker run -d -p 3000:3000 --name backend --network my_custom_network image_tag

Start mongo on the same network
    - docker run -d -v volume_database:/data/db --name mongo --network my_custom_network -p 27017:27017 mongo

    port mapping is not necessary as the containers are not communicating through the host, they are connected via network.

Check the logs to ensure the db connection is successful
    - docker logs <container_id>

Try to visit an endpoint and ensure you are able to talk to the database

If you want, you can remove the port mapping for mongo since you don’t necessarily need it exposed on your machine

Types of networks =>

    - Bridge: The default network driver for containers. When you run a container without specifying a network, it's attached to a bridge network. It provides a private internal network on the host machine, and containers on the same bridge network can communicate with each other.

    - Host: Removes network isolation between the container and the Docker host, and uses the host's networking directly. This is useful for services that need to handle lots of traffic or need to expose many ports.

--------

we were able to connect to mongo, postgres containers from our machine because we have port mapping. If we make a container for the Express application then we need to connect them to the common n/w in order for them to communicate.

the data in the container persists untill we kill the container. through start and stop the data will persist.

volume concept is limited to data being persisted locally, it can't be pushed to dockerhub or something.In kubernetes we have persistent volumes.

In a container, in /data/db folder the mongo db data is being stored,so we should attach it to the volume. for other containers it can be different (postgres or other).

RUN - runs when the image is being built
CMD - runs when the container starts

docker shines when we have 5-6 services to start (FE,BE,DB,Redis, Auxillary Service). generally for simple applications we don't need docker / kubernetes.

Docker cache invalidation and checksum calculation : docker checks file changes across image builds. it determines whether a file has changed and whether the cache needs to be invalidated by calculating a checksum for each file involved in COPY or ADD instructions in a Dockerfile. Calculate checksum based on file content for every file.

----

More Docker Commands =>

- docker exec <container_id> <command>(ls,pwd etc)
- docker exec -it <container_id> /bin/bash = creates a bash session inside the container. opens in interactive mode

----

Pushing to dockerhub

Once you’ve created your image, you can push it to dockerhub to share it with the world.

    - Signup to dockerhub
    - Create a new repository

- Login to docker cli

    - docker login

you might have to create an access token - https://docs.docker.com/security/for-developers/access-tokens/

Push to the repository

    - docker push your_username/your_reponame:tagname

    before this we need to builg the image using this your_username/your_reponame. If we are not giving any tags then it will auto pushed as latest tag. Otherwise we can give desired tags to it.

    We can also pull and run older versions by mentioning the versions in docker CLI.

Benefits =>

Github push(everytime our code changes - CI/CD config) --> spawns an ubuntu instance --> instance push the image to dockerhub => from there our kubernetes cluster can pull the image and start the container. - more on this later

----

docker-compose -->

Docker Compose is a tool designed to help you define and run multi-container Docker applications. With Compose, you use a YAML file to configure your application's services, networks, and volumes. Then, with a single command, you can create and start all the services from your configuration.

Before docker-compose -->

Create a network
    - docker network create my_custom_network
Create a volume
    - docker volume create volume_database
Start mongo container
    - docker run -d -v volume_database:/data/db --name mongo --network my_custom_network  mongo
Start backend container
    - docker run -d -p 3000:3000 --name backend --network my_custom_network backend

After docker-compose -->

Install docker-compose - https://docs.docker.com/compose/install/

Create a yaml file describing all your containers and volumes (by default all containers in a docker-compose run on the same network)

```

docker-compose.yaml --

version: '3.8'
services:
  mongodb:
    image: mongo
    container_name: mongodb
    ports:
      - "27017:27017"
    # attaches the volume
    volumes:
      - mongodb_data:/data/db

  backend22:
    image: backend
    container_name: backend_app
    # only start after this service started, if depends on other services then we can add them too
    depends_on: 
      - mongodb
    # we can add arrays as well if we want to map multiple ports
    ports: 
      - "3000:3000"
    # This value will override the default value of MONGO_URL, we should put the environment variables here.
    environment:
      MONGO_URL: "mongodb://mongodb:27017"

# creates the volume
volumes:
  mongodb_data:

```

Start the compose => (have to be in the same directory as docker-compose.yaml file)

    - docker-compose up
Stop everything (including volumes)

    - docker-compose down --volumes

-----------

In docker-compose.yaml we can define all our services, networks and volumes in a single file and run it with a single command.

If we want to build the backend from the dockerfile =>
    then instead of image we can write =>
        build: . => will rebuild the image from the Dockerfile in the parent folder(.) when we do "docker-compose up".

-----

