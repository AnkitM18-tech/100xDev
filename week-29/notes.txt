Volumes in docker =>
Pretext
The following docker image runs a Node.js app that writes peridically to the filesystem - 
https://hub.docker.com/r/100xdevs/write-random

NodeJs Code ->

const fs = require('fs');
const path = require('path');

// Function to generate random data
function generateRandomData(length) {
    let characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';
    let result = '';
    for (let i = 0; i < length; i++) {
        result += characters.charAt(Math.floor(Math.random() * characters.length));
    }
    return result;
}

// Write random data to a file
function writeRandomDataToFile(filePath, dataLength) {
    const data = generateRandomData(dataLength);
    fs.writeFile(filePath, data, (err) => {
        if (err) {
            console.error('Error writing to file', err);
        } else {
            console.log('Data written to file', filePath);
        }
    });
}

// Define the file path and data length
const filePath = path.join(__dirname, '/generated/randomData.txt');
const dataLength = 100; // Change this to desired length of random data

// Write random data to file every 10 seconds
setInterval(() => {
    writeRandomDataToFile(filePath, dataLength);
}, 10000); // 10000 ms = 10 seconds

// Keep the script running
console.log('Node.js app is running and writing random data to randomData.txt every 10 seconds.');

Run it in docker
Try running the image above in your local machine
docker run 100xdevs/write-random

Try going to the container and seeing the contents of the container
docker exec -it container_id /bin/bash
cat randomData.txt

Where is this file being stored?
The data is stored in the docker runtime filesystem . When the container dies, the data dies with it. This is called ephemeral storage

If you want to persist data across container stops and starts, you can use Volumes in Docker

Bind mounts ->
Replace the mount on the left with a folder on your own machine

docker run -v D:\\100xDev\\week-29\\class-5-k8s\\generated:/usr/src/app/generated 100xdevs/write-random

whatever stored here /usr/src/app/generated inside the container, will also get stored in D:\100xDev\week-29\class-5-k8s\generated folder on my machine.

Volume Mounts

Create a volume
docker volume create hello

Mount data to volume
docker run -v hello:/usr/src/app/generated 100xdevs/write-random

If you stop the container in either case, the randomFile.txt file persists

Volumes in kubernetes =>

Ref - https://kubernetes.io/docs/concepts/storage/volumes/

Volumes ->

In Kubernetes, a Volume is a directory, possibly with some data in it, which is accessible to a Container as part of its filesystem. Kubernetes supports a variety of volume types, such as EmptyDir, PersistentVolumeClaim, Secret, ConfigMap, and others.

Why do you need volumes?
- If two containers in the same pod want to share data/fs.
eg. Inside Cluster -> Nodes -> Inside Node -> pod -> Inside pod we have containers that want to share data.
c1,c2 -> ephemeral volume

- If you want to create a database that persists data even when a container restarts (creating a DB)
eg. Inside Cluster -> Nodes -> Inside Node -> pod -> Persistent Volume 1(AWS) => 10gb PVC

- Your pod just needs extra space during execution (for caching lets say) but doesnt care if it persists or not.
eg. Some Place inside the Node. c1 -> volume inside Node.

Types of volumes =>
- Ephemeral Volume
Temporary volume that can be shared amongst various containers of a pod.  When the pods dies, the volume dies with it.
For example - 
    ConfigMap
    Secret
    emptyDir

- Persistent Volume => exist outside of our k8s cluster
A Persistent Volume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

- Persistent volume claim =>
A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only).


DevOps administrators can create PVs just like clusters and nodes etc.

After creating a PV, we as developer can create a PVC to be attached to it.
Then a whenever a pod starts we can mount it over this PVC.

Ephemeral volumes =>

A lot of times you want two containers in a pod to share data. 
But when the pods dies, then the data can die with it. 

Setup
Create a manifest that starts two pods which share the same volume

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shared-volume-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: shared-volume-app
  template:
    metadata:
      labels:
        app: shared-volume-app
    spec:
      containers:
      - name: writer
        image: busybox
        command: ["/bin/sh", "-c", "echo 'Hello from Writer Pod' > /data/hello.txt; sleep 3600"]
        volumeMounts:
        - name: shared-data
          mountPath: /data
      - name: reader
        image: busybox
        command: ["/bin/sh", "-c", "cat /data/hello.txt; sleep 3600"]
        volumeMounts:
        - name: shared-data
          mountPath: /data
      volumes:
      - name: shared-data
        emptyDir: {}

```
Apply the manifest
    kubectl apply -f kube.yml

Check the reader container and see if you can see the volume data in there
    kubectl exec -it shared-volume-deployment-74d67d6567-tcdsl --container reader sh 


emptyDir -> starts with the pod and when the pod dies it dies with it.

# If we create 2 replicas, then 2 pods will be created and containers in each pod will have their own shared ephemeral volume.


Persistent volumes =>

Just like our kubernetes cluster has nodes where we provision our pods.
We can create peristent volumes where our pods can claim (ask for) storage

Persistent volumes can be provisioned statically or dynamically.

Static persistent volumes =>

Creating a NFS
NFS is one famous implementation you can use to deploy your own persistent volume

I’m running one on my aws server - 

```

version: '3.7'

services:
  nfs-server:
    image: itsthenetwork/nfs-server-alpine:latest
    container_name: nfs-server
    privileged: true
    environment:
      SHARED_DIRECTORY: /exports
    volumes:
      - ./data:/exports:rw
    ports:
      - "2049:2049"
    restart: unless-stopped

Make sure the 2049 port on your machine is open
```
Creating a pv and pvc
Create a persistent volume claim and persistent volume

```

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  storageClassName: nfs
  nfs:
    path: /exports
    server: 52.66.197.168
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs

```

Create a pod =>

```

apiVersion: v1
kind: Pod
metadata:
  name: mongo-pod
spec:
  containers:
  - name: mongo
    image: mongo:4.4
    command: ["mongod", "--bind_ip_all"]
    ports:
    - containerPort: 27017
    volumeMounts:
    - mountPath: "/data/db"
      name: nfs-volume
  volumes:
  - name: nfs-volume
    persistentVolumeClaim:
      claimName: nfs-pvc

```

NFS -> Networked File System -> PV deployment implementation

k8's cluster 

we will create an EC2 instance, inside which we will have docker running our NFS.

and since docker container is not persistent, we can mount our data into a folder on the EC2 instance.
So whatever data the NFS is storing, we will back it up in that mounted folder.

Another approach would be we can run NFS directly in our EC2 instance. Then expose NFS protocol via a port on the EC2 instance and connect our k8s cluster to it.

Or we can opt for block storage from cloud providers instead of deploying our PV ourselves.

Then we can create a PV in k8s cluster which has access to the NFS, so that eventually the developers can make PVC.

PV ->
```

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  storageClassName: nfs
  nfs:
    path: /exports
    server: 52.66.197.168 -> IP of EC2 server

# storageClassName -> this changes as the storage provider changes (self deployed nfs/aws/gcp/vultr block storage)

path: /exports -> the folder which we created inside the EC2 NFS server.


```

```

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs
---
apiVersion: v1
kind: Pod
metadata:
  name: mongo-pod
spec:
  containers:
    - name: mongo
      image: mongo:4.4
      command: ["mongod", "--bind_ip_all"]
      ports:
        - containerPort: 27017
      volumeMounts:
        - mountPath: "/data/db" - the place where the mongodb data gets stored
          name: nfs-volume
  volumes:
    - name: nfs-volume
      persistentVolumeClaim:
        claimName: nfs-pvc


Only when the PVC gets mount to the PV, then only the pod will start. If there are no PVs available which can handle the constraints of the PVC, then the pod will not start.


```
when a PVC gets bound to a PV, no one else can claim it.

PVC bound to the PV, is volume of the pod starting.

The folder in the NFS server is mounted to the PV. PV inside the k8s cluster is bound to the PVC. Eventually the PVC becomes the volume of the pod.


Automatic pv creation =>

Ref - https://docs.vultr.com/how-to-provision-persistent-volume-claims-on-vultr-kubernetes-engine

Create a persistent volume claim with storageClassName set to vultr-block-storage-hdd

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: csi-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Gi
  storageClassName: vultr-block-storage-hdd


Apply the pod manifest =>

apiVersion: v1
kind: Pod
metadata:
  name: mongo-pod
spec:
  containers:
  - name: mongo
    image: mongo:4.4
    command: ["mongod", "--bind_ip_all"]
    ports:
    - containerPort: 27017
    volumeMounts:
    - name: mongo-storage
      mountPath: /data/db
  volumes:
  - name: mongo-storage
    persistentVolumeClaim:
      claimName: csi-pvc

Explore the resources created ->

kubectl get pv
kubectl get pvc
kubectl get pods

Put some data in mongodb ->

kubectl exec -it mongo-pod -- mongo
use mydb
db.mycollection.insert({ name: "Test", value: "This is a test" })
exit

Delete and restart the pod ->

kubectl delete pod mongo-pod
kubectl apply -f mongo.yml

Check if the data persists ->

kubectl exec -it mongo-pod -- mongo
use mydb
db.mycollection.find()

Just like how the cloud provider creates a LB outside the cluster, likewise the developer doesn't need to worry about the PV provisioning. Developer just have to write manifest for PVC and PV will get automatically created by the cloud provider.


We can define and mount multiple volumes to our pod.(secrets, configmaps etc)

A single PVC we can connect to multiple pods as well.

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs
---
apiVersion: v1
kind: Pod
metadata:
  name: mongo-pod
spec:
  containers:
    - name: mongo
      image: mongo:4.4
      command: ["mongod", "--bind_ip_all"]
      ports:
        - containerPort: 27017
      volumeMounts:
        - mountPath: "/data/db"
          name: nfs-volume
        - mountPath: "/data/config"
          name: config-volume
  volumes:
    - name: nfs-volume
      persistentVolumeClaim:
        claimName: nfs-pvc
    - name: config-volume
      secret:
        secretName: mongo-secret-config

```

PV => PVC mapping is done on cluster basis, so people having their own k8s cluster can connect to the publicly available NFS and read and write to it.


----------------------------------------

What we’re learning =>

HPA - Horizontal Pod Autoscaling
Node Autoscaling
Resource management

Horizontal pod Autoscaler =>
Ref - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

- A Horizontal Pod Autoscaler (HPA) is a Kubernetes feature that automatically adjusts the number of pod replicas in a deployment, replica set, or stateful set based on observed metrics like CPU utilisation or custom metrics.

- This helps ensure that the application can handle varying loads by scaling out (adding more pod replicas) when demand increases and scaling in (reducing the number of pod replicas) when demand decreases.

Horizontal scaling =>
- As the name suggests, if you add more pods to your cluster, it means scaling horizontally. Horizontally refers to the fact that you havent increased the resources on the machine.


Architecture =>

Kubernetes implements horizontal pod autoscaling as a control loop that runs intermittently (it is not a continuous process) (once every 15s)
 
- cadvisor -  https://github.com/google/cadvisor
- Metrics server - The Metrics Server is a lightweight, in-memory store for metrics. It collects resource usage metrics (such as CPU and memory) from the kubelets and exposes them via the Kubernetes API (Ref - https://github.com/kubernetes-sigs/metrics-server/issues/237)


kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
or
Apply from here - https://github.com/100xdevs-cohort-2/week-28-manifests


Try getting the metrics ->

kubectl top pod -n kube-system
kubectl top nodes -n kube-system

Sample request that goes from hpa controller to the API server ->
- GET https://338eb37e-2824-4089-8eee-5a05f84fb85e.vultr-k8s.com:6443/apis/metrics.k8s.io/v1beta1/namespaces/default/pods


HPA takes care of the replicas to be created and reapply the manifest according to the traffic.

k8s cluster =>

Master Node
- API Server - hpa controller loops once in 15s.


Worker Node 
  - kubelet
  - cadvisor - tells kubelet to store the stat in metrics server
  - metrics server - we have to add in our node, takes stats from every worker node and host them on the API server on master node.
  - pod 1 - sends pod level metrics to cadvisor


k8s by default expose the pod level metrics but don't store them, so we need to add the metrics server.

Better metrics for autoscaling are -> cpu usage and memory usage.

hpa controller keeps on checking the metrics coming from worker nodes to the API server every 15s and if anything changes then it will scale up or scale down pods.

hpa controller comes by default, but metrics server we need to add in the worker nodes.

This metric server aggregates all metrics like avg cpu usage, memory usage and all. Then it exposes it to the API server.


App for the day =>

We’ll be creating a simple express app that does a CPU intensive task to see horizontal scaling in action.

```

import express from 'express';

const app = express();
const BIG_VALUE = 10000000000;

app.get('/', (req, res) => {
    let ctr = 0;
    for (let i = 0; i < BIG_VALUE; i++) {
        ctr += 1;
    }
    res.send('Hello World!');
});

app.listen(3000, () => {
  console.log('Server is running on port 3000');
});

```

The app is deployed at https://hub.docker.com/r/100xdevs/week-28

Creating the manifests =>

Hardcoded replicas
- Lets try to create a deployment with hardcoded set of replicas

```

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-app
  template:
    metadata:
      labels:
        app: cpu-app
    spec:
      containers:
      - name: cpu-app
        image: 100xdevs/week-28:latest
        ports:
        - containerPort: 3000

```
Create a service ->

apiVersion: v1
kind: Service
metadata:
  name: cpu-service
spec:
  selector:
    app: cpu-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
  type: LoadBalancer


With a horizontal pod autoscaler ->

Add HPA manifest

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cpu-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cpu-deployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50


Apply all three manifests
  kubectl apply -f service.yml
  kubectl apply -f deployment.yml
  kubectl apply -f hpa.yml

You can scale up/down based on multiple metrics. 
If either of the metrics goes above the threshold, we scale up
If all the metrics go below the threshold, we scale down.


Scaling up =>

Before we load test, add some resource limits to your pods. We’re doing this to get around this error -  https://github.com/kubernetes-sigs/metrics-server/issues/237

```

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-app
  template:
    metadata:
      labels:
        app: cpu-app
    spec:
      containers:
      - name: cpu-app
        image: 100xdevs/week-28:latest
        ports:
        - containerPort: 3000
        resources:
          requests:
            cpu: "100m"
          limits:
            cpu: "1000m"

```

Try sending a bunch of requests to the server (or just visit it in the browser)
  npm i -g loadtest
  loadtest -c 10 --rps 200 http://65.20.89.70

Check the CPU usage
  kubectl top pods   

See the hpa average usage
  kubectl get hpa

Check the number of pods, see as they scale up
  kubectl get pods


Formula for scaling up =>

new desired capacity = total cpu util/target cpu

ex-1
pod 1 - 70%
pod 2 - 80%
target = 50%
(70+80)/50 = 3

ex-2
pod 1 = 40%
pod 2 = 20%
pod 3 = 10%
target = 50%
(40+20+10)/50 = 2

----------------------------------------------

Resource requests and limits =>
- Ref - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
- When you specify a Pod, you can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and memory (RAM).

- There are two types of resource types

- Resource requests =>
  - The kubelet reserves at least the request amount of that system resource specifically for that container to use.
- Resource limits =>
  - When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set. 


Difference b/w limits and requests =>
  - If the node where a Pod is running has enough of a resource available, it's possible (and allowed) for a container to use more resource than its request for that resource specifies. However, a container is not allowed to use more than its resource limit.


Experiment ->

30% CPU usage on a single threaded Node.js app =>
  - Update the spec from the last slide to decrease the CPU usage. Notice that the CPU doesnt go over 30% even though this is a Node.js app where it can go up to 100%

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-app
  template:
    metadata:
      labels:
        app: cpu-app
    spec:
      containers:
      - name: cpu-app
        image: 100xdevs/week-28:latest
        ports:
        - containerPort: 3000
        resources:
          requests:
            cpu: "100m"
          limits:
            cpu: "300m"
```

Request 2 vCPU in 10 replicas =>
  - Try requesting more resources than available in the cluster.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-deployment
spec:
  replicas: 10
  selector:
    matchLabels:
      app: cpu-app
  template:
    metadata:
      labels:
        app: cpu-app
    spec:
      containers:
      - name: cpu-app
        image: 100xdevs/week-28:latest
        ports:
        - containerPort: 3000
        resources:
          requests:
            cpu: "1000m"
          limits:
            cpu: "1000m"

```

Auto scaling pods/group is bad when we have spiky workloads.

Cluster autoscaling =>
Ref - https://github.com/kubernetes/autoscaler

Cluster Autoscaler - a component that automatically adjusts the size of a Kubernetes Cluster so that all pods have a place to run and there are no unneeded nodes. Supports several public cloud providers. Version 1.0 (GA) was released with kubernetes 1.8.

Underprovisioned resources ->
In the last slide, we saw that we didn’t have enough resources to schedule a pod on.

Let’s make our node pool dynamic and add a min  and max nodes.

Restart the deployment ->
  kubectl delete deployment cpu-deployment
  kubectl apply -f deployment.yml

Notice a new node gets deployed

Logs of the cluster autoscaler
  kubectl get pods -n kube-system | grep cluster-autoscaler

Try downscaling =>

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-deployment
spec:
  replicas: 10
  selector:
    matchLabels:
      app: cpu-app
  template:
    metadata:
      labels:
        app: cpu-app
    spec:
      containers:
      - name: cpu-app
        image: 100xdevs/week-28:latest
        ports:
        - containerPort: 3000
        resources:
          limits:
            cpu: "1000m"
          requests:
            cpu: "1000m"

```

Notice the number of server goes down to 2 again

Good things to learn after this - 
- Gitops (ArgoCD)
- Custom metrics based scaling, event based autoscaling - https://www.giffgaff.io/tech/event-driven-autoscaling
- Deploying prometheus in a k8s cluster, scaling based on custom metrics from prometheus
 

Kubernetes Lab =>
 
Base repository - https://github.com/code100x/algorithmic-arena/

Things to do - 
  - Create a PV, PVC for redis.
  - Create a PV, PVC for the postgres database.
  - Create deployments for redis, postgres.
  - Create ClusterIP services for redis, postgres
  - Create a deployment for the nextjs app, expose it via a loadbalancer service on ‣
  - Create a deployment for the judge api server. Expose it via a ClusterIP service
  - Create a deployment for the judge workers. Add resource limits and requests to it
  - Create a HPA that scales based on the pending submission queue length in the redis queue
      - You can either expose an endpoint that you use as a custom metric
      - You can put all metrics in prometheus and pick them up from there
      - You can use KEDA to scale based on redis queue length

















