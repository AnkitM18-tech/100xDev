What we’ll be doing today
What we’ve done already ->
    Clusters
    Nodes
    Pods
    Deployment
    Replicasets
 
What we’re doing today ->
    Namespaces
    Ingress
    Ingress Controller
    nginx
    traefik
    ConfigMaps
    Secrets
 
What we’re doing tomorrow ->
    Cert management
    Volumes and Persistent volumes
    Resource management 
 
Offline video next week ->
    HPA - Horizontal Pod Autoscaling
    Node autoscaling
    Labs to add k8s a real codebase

Recapping what we’ve done
Ref - https://projects.100xdevs.com/tracks/kubernetes-1/Kubernetes-Part-1-1

Cluster
A kubernetes cluster is a bunch of machines that work together to help you deploy your app

Nodes
Each machine in your cluster is called a node
Nodes are of two types
Master node (control plane) - Exposes an API that the developer can use to deploy pods
Worker node - Actually run the pods

Pods
The smallest execution unit inside a kubernetes clusters. A pod can run one or more containers

Replicasets
They let you create multiple pods (replicas). 
It also takes care of bringing them back up if they ever go down/are killed

Deployment
A deployment creates replicasets.

Services let you expose your pods to other pods/over the internet
They are of three types
ClusterIP -> internal communication
NodePort -> exposes a port to the outside world
Loadbalancer — Creates a loadbalancer outside the kubernetes cluster

Recapping how to run this locally =>

Creating a cluster
Create a kind.yml file locally ->

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30007
    hostPort: 30007
- role: worker
  extraPortMappings:
  - containerPort: 30007
    hostPort: 30008
- role: worker

Run the cluster locally ->
 kind create cluster --config kind.yml --name local2

Run docker ps to confirm that the cluster is running ->
 docker ps

Creating a pod =>
Create a pod manifest (pod.yml) ->

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80

Apply the pod manifest
    - kubectl apply -f pod.yml

Check if the pod exists now
    - kube get pods

Check the logs 
    - kubectl logs -f nginx

Delete the pod
    - kubectl delete pod nginx

Creating a replicaset
Create the replicaset manifest

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80

Apply the replicaset manifest
 kubectl apply -f rs.yml

Check the number of pods running now
kubectl get pods

Try deleting a pod, and ensure it gets restarted
Delete the replicaset
 kubectl delete rs nginx-replicaset

Creating a Deployment
Create a deployment manifest (deployment.yml)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80

Apply the manifest
kubectl apply -f deployment.yml

Check the rs that exist now
kubectl get rs

Check the pods that exist now
kubectl get pods

Try creating a new deployment with a wrong image name

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx2:latest
        ports:
        - containerPort: 80

Ensure that the old pods are still running
kubectl get pods

How to run this on a cloud provider
Go to a cloud provider like
AWS
GCP
Digital ocean
Vultr
and create a k8s cluster

Download the credentials file and replace ~/.kube/config with it

Create a deployment manifest

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx2:latest
        ports:
        - containerPort: 80

Create a deployment
kubectl apply -f deployment.yml

Services

Services let you actually expose your app over the internet.

Nodeport
Create a Nodeport service (service.yml)

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30007  # This port can be any valid port within the NodePort range
  type: NodePort

Apply it
kubectl apply -f service.yml

Visit any of the nodes on 30007
http://localhost:30007/

This will only work if you’ve started your kind cluster with the config from slide 2
On vultr, it will just work

LoadBalancer (will only work on a cloud provider)

The LoadBalancer service type is designed to work with cloud providers to create an external load balancer that routes traffic to the service.

Replace the type to be LoadBalancer

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

Re-apply the config
kubectl apply -f service.yml

See the loadbalancer created on the dashboard
Visit the balancer to see the website
Ideal way to expose nodes to the internet is via LoadBalancer.

We have to tell the deployment the pods it owns are tagged to a label(app => nginx here) when they are starting.

So that it can keep track of the pods, if all the pods are running and healthy or not.

selector: -- this does that tagging and matching of pods for deployment to keep track
    matchLabels:
        app: nginx

metadata: -- this set the label tag of pods to nginx
    labels:
        app: nginx


the loadbalancer IP doesn't need the NodePort mapping, only the Node Port service requires the NodeIP : exposedPort

By using LoadBalancer service, we don't need to expose the node IP.


Downsides of services =>

Services are great, but they have some downsides - 

- Scaling to multiple apps
    - If you have three apps (frontend, backend, websocket server), you will have to create 3 saparate services to route traffic to them. There is no way to do centralized traffic management (routing traffic from the same URL/Path-Based Routing) 
    - There are also limits to how many load balancers you can create

- Multiple certificates for every route
    - You can create certificates for your load balancers but you have to maintain them outside the cluster and create them manually
    - You also have to update them if they ever expire

- No centralized logic to handle rate limitting to all services
    - Each load balancer can have its own set of rate limits, but you cant create a single rate limitter for all your services. 

Trying it out
Here is a sample manifest that you can run to start two saparate deployments and attach them to two saparate LoadBalancer services

Manifest =>

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: my-apache-site
        image: httpd:2.4
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: apache-service
spec:
  selector:
    app: apache
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

kubectl apply -f manifest.yml

You will notice two load balancers created for your two services
Open the load balancers IP and see they work.

Ingress and Ingress Controller =>

Ref - https://kubernetes.io/docs/concepts/services-networking/ingress/

An API object that manages external access to the services in a cluster, typically HTTP.
Ingress may provide load balancing, SSL termination and name-based virtual hosting.

Ingress exposes HTTP and HTTPS route from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.

Ex: where an Ingress sends all its traffic to one service.

client -> Ingress Managed LB -> |(cluster) Ingress-- Routing rule -> Service --> Pods


An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer.

Controller Manager inside the Master node runs a bunch of controllers under the hood.
    - Deployment controller
    - ReplicaSet controller

Ingress Controller is not there by default. We have to install one of the ingress controllers.
    - nginx ingress controller
    - haproxy ingress controller
    - traefik ingress controller

Suppose we have a cluster -> inside we have nodes ->
in which we have FE pods and BE pods.

Any request that is coming to the LB -> will go to a pod that is running an ingress controller (LB library)
Ingress controller will check where the request is coming from, if it is coming from 

api.100xdevs.com -> redirect to any of BE pods
100xDevs.com -> redirect to any of FE pods

In those nodes we have clusterip service, ingress controller will send the request to the clusterip service and it will get load balanced among pods.

In this case we need to create ->

Deployment for FE
Deployment for BE
ClusterIP service for FE
ClusterIP service for BE
Ingress

Whenever we add an ingress controller, it automatically adds a ingress-managed LB outside the cluster.

Ingress controller =>

If you remember from last week, our control plane had a controller manager running.
Ref - https://projects.100xdevs.com/tracks/kubernetes-1/Kubernetes-Part-1-3


The kube-controller-manager runs a bunch of controllers like ->
    Replicaset controller
    Deployment controller
    etc

If you want to add an ingress to your kubernetes cluster, you need to install an ingress controller manually. It doesn’t come by default in k8s.

Famous k8s ingress controllers ->

    - The NGINX Ingress Controller for Kubernetes works with the NGINX webserver (as a proxy).
    - HAProxy Ingress is an ingress controller for HAProxy.
    - The Traefik Kubernetes Ingress provider is an ingress controller for the Traefik proxy.

Full list - https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/


Namespaces =>

In Kubernetes, a namespace is a way to divide cluster resources between multiple users/teams. Namespaces are intended for use in environments with many users spread across multiple teams, or projects, or environments like development, staging, and production.

When you do 
    - kubectl get pods
it gets you the pods in the default namespace

Creating a new namespace
Create a new namespace
    - kubectl create namespace backend-team
Get all the namespaces
    - kubectl get namespaces
Get all pods in the namespace
    - kubectl get pods -n my-namespace

Create the manifest for a deployment in the namespace =>

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: backend-team
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80

Apply the manifest
    - kubectl apply -f deployment-ns.yml
Get the deployments in the namespace
    - kubectl get deployment -n backend-team
Get the pods in the namespace
    - kubectl get pods -n backend-team
Set the default context to be the namespace
    - kubectl config set-context --current --namespace=backend-team
Try seeing the pods now
    - kubectl get pods
Revert back the kubectl config
    - kubectl config set-context --current --namespace=default


Install the nginx ingress controller ->

Ref - https://docs.nginx.com/nginx-ingress-controller/installation/installing-nic/installation-with-manifests/

Using helm
Install helm
Ref - https://helm.sh/
Installation - https://helm.sh/docs/intro/install/

Add the ingress-nginx chart
    - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
    - helm repo update
    - helm install nginx-ingress ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace

Check if you have pods running in the 
    - kubectl get pods -n ingress-nginx

Default loadbalancer service
    - You will notice that if you use helm  to install the nginx-ingress-controller, it creates a Loadbalancer service for you
    - kubectl get services --all-namespaces

This routes all the traffic to an nginx pod
    - kubectl get pods -n ingress-nginx

This means the first part of our ingress deployment is already created

T segregate pods on the basis of teams, we can attach a namespace to it. Otherwise by default it will be created in "default" namespace.

Adding the routing to the ingress controller
Next up, we want to do the following - 

client --> LB --> Ingress Pod --> service --> redirect to respective pod.


Get rid of all existing deployments in the default namespace
    kubectl get deployments
    kubectl delete deployment_name

Get rid of all the services in the default namespace (dont delete the default kubernetes service, delete the old nginx and apache loadbalancer services)
    kubectl get services

Create a deployment and service definition for the nginx image/app (this is different from the nginx controller)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: default
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP


Create a deployment and service for the apache app

apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: my-apache-site
        image: httpd:2.4
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: apache-service
  namespace: default
spec:
  selector:
    app: apache
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP


Create the ingress resource

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-apps-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: your-domain.com
    http:
      paths:
      - path: /nginx
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
      - path: /apache
        pathType: Prefix
        backend:
          service:
            name: apache-service
            port:
              number: 80

Combined manifest
Create a combined manifest with all the api objects

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: default
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: my-apache-site
        image: httpd:2.4
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: apache-service
  namespace: default
spec:
  selector:
    app: apache
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-apps-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: your-domain.com
    http:
      paths:
      - path: /nginx
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
      - path: /apache
        pathType: Prefix
        backend:
          service:
            name: apache-service
            port:
              number: 80

Apply the manifest
kubectl apply -f complete.yml

Update your local hosts entry (/etc/hosts  ) such that your-domain.com points to the IP of your load balancer
65.20.84.86	your-domain.com

Try going to your-domain.com/apache and your-domain.com/nginx


Trying traefik’s ingress controller =>

Traefik is another popular ingress controller. Let’s try to our apps using it next

Install traefik ingress controller using helm ->

helm repo add traefik https://helm.traefik.io/traefik
helm repo update
helm install traefik traefik/traefik --namespace traefik --create-namespace

Make sure an ingressClass is created for traefik
    - kubectl get IngressClass

Notice it created a LoadBalancer svc for you
    - kubectl get svc -n traefik

Create a Ingress that uses the traefik ingressClass and traefik annotations (traefik.yml)

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: traefik-web-apps-ingress
  namespace: default
spec:
  ingressClassName: traefik
  rules:
  - host: traefik-domain.com
    http:
      paths:
      - path: /nginx
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
      - path: /apache
        pathType: Prefix
        backend:
          service:
            name: apache-service
            port:
              number: 80

Add an entry to your /etc/hosts  (IP should be your loadbalancer IP)
65.20.90.183    traefik-domain.com

Visit the website
traefik-domain.com/nginx
traefik-domain.com/apache

Can you guess what is going wrong? Why are you not seeing anything on this final page?

Ingress is a resource that user creates, and ingress controller -> is an implementation that can handle your ingress.

Secrets and configmaps

Kubernetes suggests some standard configuration practises.
These include things like
    You should always create a deployment rather than creating naked pods
    Write your configuration files using YAML rather than JSON
    Configuration files should be stored in version control before being pushed to the cluster
 
Kubernetes v1 API also gives you a way to store configuration of your application outside the image/pod
This is done using 
    ConfigMaps
    Secrets

Rule of thumb
Don’t bake your application secrets in your docker image
Pass them in as environment variables whenever you’re starting the container

ConfigMaps =>

Ref - https://kubernetes.io/docs/concepts/configuration/configmap/
A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.


Creating a ConfigMap =>

Create the manifest ->

apiVersion: v1
kind: ConfigMap
metadata:
  name: ecom-backend-config
data:
  database_url: "mysql://ecom-db:3306/shop"
  cache_size: "1000"
  payment_gateway_url: "https://payment-gateway.example.com"
  max_cart_items: "50"
  session_timeout: "3600"

Apply the manifest
	kubectl apply -f cm.yml

Get the configmap
    kubectl describe configmap ecom-backend-config

Creating an express app that exposes env variables


import express from 'express';
import fs from 'fs';
import path from 'path';

const app = express();
const port = 3000;
app.get('/', (req, res) => {
  const envVars = {
    DATABASE_URL: process.env.DATABASE_URL,
    CACHE_SIZE: process.env.CACHE_SIZE,
    PAYMENT_GATEWAY_URL: process.env.PAYMENT_GATEWAY_URL,
    MAX_CART_ITEMS: process.env.MAX_CART_ITEMS,
    SESSION_TIMEOUT: process.env.SESSION_TIMEOUT,
  };

  res.send(`
    <h1>Environment Variables</h1>
    <pre>${JSON.stringify(envVars, null, 2)}</pre>
  `);
});

app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`);
});


Deploy to dockerhub - https://hub.docker.com/repository/docker/100xdevs/env-backend/general

Trying the express app using docker locally ->

 docker run -p 3003:3000 -e DATABASE_URL=asd  100xdevs/env-backend

Try running using k8s locally
Create the manifest (express-app.yml)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ecom-backend-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ecom-backend
  template:
    metadata:
      labels:
        app: ecom-backend
    spec:
      containers:
      - name: ecom-backend
        image: 100xdevs/env-backend
        ports:
        - containerPort: 3000
        env:
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: ecom-backend-config
              key: database_url
        - name: CACHE_SIZE
          valueFrom:
            configMapKeyRef:
              name: ecom-backend-config
              key: cache_size
        - name: PAYMENT_GATEWAY_URL
          valueFrom:
            configMapKeyRef:
              name: ecom-backend-config
              key: payment_gateway_url
        - name: MAX_CART_ITEMS
          valueFrom:
            configMapKeyRef:
              name: ecom-backend-config
              key: max_cart_items
        - name: SESSION_TIMEOUT
          valueFrom:
            configMapKeyRef:
              name: ecom-backend-config
              key: session_timeout

Apply the manifest
    kubectl apply -f express-app.yml

Create the service (express-service.yml)

apiVersion: v1
kind: Service
metadata:
  name: ecom-backend-service
spec:
  type: NodePort
  selector:
    app: ecom-backend
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30007

Apply the service
    kubectl apply -f express-service.yml


Secrets =>

Secrets are also part of the kubernetes v1 api. They let you store passwords / sensitive data which can then be mounted on to pods as environment variables. Using a Secret means that you don't need to include confidential data in your application code.
Ref - https://kubernetes.io/docs/concepts/configuration/secret/

Using a secret
Create the manifest with a secret and pod (secret value is base64 encoded) (https://www.base64encode.org/)

apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  .env: REFUQUJBU0VfVVJMPSJwb3N0Z3JlczovL3VzZXJuYW1lOnNlY3JldEBsb2NhbGhvc3QvcG9zdGdyZXMi
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-dotfiles-pod
spec:
  containers:
    - name: dotfile-test-container
      image: nginx
      volumeMounts:
        - name: env-file
          readOnly: true
          mountPath: "/etc/secret-volume"
  volumes:
    - name: env-file
      secret:
        secretName: dotfile-secret

Try going to the container and exploring the .env

kubectl exec -it secret-dotfiles-pod /bin/bash
cd /etc/secret-volume/
ls


Base64 encoding =>

Whenever you’re storing values in a secret, you need to base64 encode them. They can still be decoded , and hence this is not for security purposes. This is more to provide a standard way to store secrets, incase they are binary in nature. 

For example, TLS (https) certificates that we’ll be storing as secrets eventually can have non ascii characters. Converting them to base64 converts them to ascii characters.
 
Secrets as env variables
You can also pass in secrets as environment variables to your process (similar to how we did it for configmaps in the last slide)

Create the secret

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
data:
  username: YWRtaW4=  # base64 encoded 'admin'
  password: cGFzc3dvcmQ=  # base64 encoded 'password'


Create the pod

apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: my-container
    image: busybox
    command: ["/bin/sh", "-c", "echo Username: $USERNAME; echo Password: $PASSWORD; sleep 3600"]
    env:
    - name: USERNAME
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: username
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: password


ConfigMaps vs Secrets

Creating a ConfigMap

apiVersion: v1
kind: ConfigMap
metadata:
  name: example-config
data:
  key1: value1
  key2: value2

Creating a Secret

apiVersion: v1
kind: Secret
metadata:
  name: example-secret
data:
  password: cGFzc3dvcmQ=
  apiKey: YXBpa2V5

Key differences ->

- Purpose and Usage:
    Secrets: Designed specifically to store sensitive data such as passwords, OAuth tokens, and SSH keys.
    ConfigMaps: Used to store non-sensitive configuration data, such as configuration files, environment variables, or command-line arguments.
- Base64 Encoding:
    Secrets: The data stored in Secrets is base64 encoded. This is not encryption but simply encoding, making it slightly obfuscated. This encoding allows the data to be safely transmitted as part of JSON or YAML files.
    ConfigMaps: Data in ConfigMaps is stored as plain text without any encoding.
- Volatility and Updates:
    Secrets: Often, the data in Secrets needs to be rotated or updated more frequently due to its sensitive nature.
    ConfigMaps: Configuration data typically changes less frequently compared to sensitive data.
- Kubernetes Features:
    Secrets: Kubernetes provides integration with external secret management systems and supports encryption at rest for Secrets when configured properly. Ref https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver
    ConfigMaps: While ConfigMaps are used to inject configuration data into pods, they do not have the same level of support for external management and encryption.

Adding https using cert-manager
Ref - https://cert-manager.io/
Try installing a certificate for a domain name of your own before tomorrows class
Maybe get a domain name from namecheap for cheap - https://www.namecheap.com/


Developer just have to create an ingress resource and write the ingressClassName and then the controller will be created to handle the LB pod.

Ingress Controller is one of many implementations that can handle your ingress.

Ingress Controller is an abstraction just like deployment controller, which is a process that keeps on running and checking the respective pods.

Try to figure out how can you rewrite the path to / if you’re using traefik as the ingress class

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myingress
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: web

spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /bar
            pathType: Exact
            backend:
              service:
                name:  whoami
                port:
                  number: 80
          - path: /foo
            pathType: Exact
            backend:
              service:
                name:  whoami
                port:
                  number: 80


Configuration files should be stored in version control before being pushed to the cluster

Ideally we should not kubectl apply configs, it should be managed by version control CI/CD.

Backend Image (no secrets) --> hub.docker.com --> Kubernetes Cluster (pass in env variables here)
--> EC2 Machine (docker run image -e DATABASE_URL=asd - shall pass like this)

It is a bad practice for our images to have any secrets or configuration params.

some of them are secrets -> jwt secret key, authentication password, database url secret

some of them are just params ->
how often do you want to scrape metrcis(15s) like this.

We should pass these from outside the application -> when we are starting our image

ConfigMaps and Secrets are there in etcd in plaintext, but we can secure this encode this if we want.

Our Image or code should not include config variables or secrets hardcoded.

While starting the app, we can pass those env variables -> 
DATABASE_URL="----" node index.js 
now if we do console.log(process.env.DATABASE_URL); // will log the env variable passed.

or we can set the env variable for the session by running export DATABASE_URL="----" => for this session DATABASE_URL is set.

Don’t bake your application secrets in your docker image
Pass them in as environment variables whenever you’re starting the container

We can give the environment variables while starting the node js process, starting a container or when we are starting a k8s cluster.

ConfigMaps are used for non-secrets, and Secrets are used for secrets.

Secrets =>
Secrets are also part of the kubernetes v1 api. They let you store passwords / sensitive data which can then be mounted on to pods as environment variables. Using a Secret means that you don't need to include confidential data in your application code.
Ref - https://kubernetes.io/docs/concepts/configuration/secret/

Using a secret
Create the manifest with a secret and pod (secret value is base64 encoded) (https://www.base64encode.org/)

apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  .env: REFUQUJBU0VfVVJMPSJwb3N0Z3JlczovL3VzZXJuYW1lOnNlY3JldEBsb2NhbGhvc3QvcG9zdGdyZXMi
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-dotfiles-pod
spec:
  containers:
    - name: dotfile-test-container
      image: nginx
      volumeMounts:
        - name: env-file
          readOnly: true
          mountPath: "/etc/secret-volume"
  volumes:
    - name: env-file
      secret:
        secretName: dotfile-secret

after adding .env file -> create .dockerignore -> add .env

then build again. and push it to dockerhub.

Visual Representation of volume mounts on how we put .env file inside the /etc/secret-volume folder in container.

Node ->
  pod ->
    container ->
      /etc/secret-volume ---> .env


Volume - env-file
.env =>

'''
deployment file =>

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ecom-backend-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ecom-backend
  template:
    metadata:
      labels:
        app: ecom-backend
    spec:
      containers:
        - name: ecom-backend
          image: 100xdevs/env-backend
          ports:
            - containerPort: 3000
          volumeMounts:
            - name: env-file
              readOnly: true
              mountPath: "/app/config"
      volumes:
        - name: env-file
          secret:
            secretName: dotfile-secret

'''

'''
secret file =>

apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  .env: REFUQUJBU0VfVVJMPSJteXNxbDovL2Vjb20tZGI6MzMwNi9zaG9wIgpDQUNIRV9TSVpFPSIxMDAwIgpQQVlNRU5UX0dBVEVXQVlfVVJMPSJodHRwczovL3BheW1lbnQtZ2F0ZXdheS5leGFtcGxlLmNvbSIKTUFYX0NBUlRfSVRFTVM9IjUwIgpTRVNTSU9OX1RJTUVPVVQ9IjM3MDAi
  cache: REFUQUJBU0VfVVJMPSJteXNxbDovL2Vjb20tZGI6MzMwNi9zaG9wIgpDQUNIRV9TSVpFPSIxMDAwIgpQQVlNRU5UX0dBVEVXQVlfVVJMPSJodHRwczovL3BheW1lbnQtZ2F0ZXdheS5leGFtcGxlLmNvbSIKTUFYX0NBUlRfSVRFTVM9IjUwIgpTRVNTSU9OX1RJTUVPVVQ9IjM3MDAi
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-dotfiles-pod
spec:
  containers:
    - name: dotfile-test-container
      image: nginx
      volumeMounts:
        - name: env-file
          readOnly: true
          mountPath: "/etc/secret-volume"
  volumes:
    - name: env-file
      secret:
        secretName: dotfile-secret

'''


we can put the mountPath inside deployment and also give data like above and we can find the config/cache inside deployment using -> kubectl exec -it <deployment_name> /bin/bash.

This will also work in our node as well. fetch from config/.env as wrtten in the code.

We can automatically push the image in dockerhub when there is a change changes,using CI/CD -> we send the image:hash and in our deployment yaml file we pull that hash and apply.











